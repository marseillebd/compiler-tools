# Devlog for Marseille's Compiler Tools

## Abstract

The problem is that I want to create programming languages, plural.
These languages should be able to share a lot of infrastructure.
These compiler tools are that shared infrastructure.

I already have `nanopass` to help write translation phases.
I am now writing "Common Conocrete Syntax" to help write parsers.
A library for representing source code generically has also proven useful.

## Common Concrete Syntax

The name is based on "Common Expression Language" (CEL).
If I had known about it earlier, it would have certainly saved me some ad-hoc, sub-optimal wheel-reinvention.
However, CEL only does expressions, but I want a system for full languages.

The "Common Concrete Syntax" (CCS) is just that system.
It aims to encode only the most useful and familiar elements of concrete syntax, and to do it generically.
It will consist of:
- a standard (WIP)
- a reference implementation (lexer is finished, parser remains)
- an API to ease downstream syntactic analysis (not started)

### Implementation

So far, only the lexer is done.
It consists of a "coverage lexer", then a number of cleanup passes, culminating in a properly lexed token stream.

The coverage lexer is designed to partition the input characters.
That is, every character (and byte) of input belongs to exactly one token.
There is only one zero-width token possible --- end of file --- but otherwise every token monotonoically increases the position.
I will expose the results of the coverage lexer because I suspect it will be useful to implement syntax highlighting in a language server.

The full lexer, by contrast, need not cover the entire input, and multiple tokens (eg indentation) might overlap each other.
However, it reduces the noise significantly for any parser operating over it (no need to worry about whitespace or comments, no need for context-sensitivity).
I don't want to expose the parts of the cleanup pipeline; everything is temporary, except exposed implementation details.

It took a few iterations to get there, but the cleanup stages consist of noise reduction, assembly, indentation detection, and sandhi.
Noise reduction gets rid of irrelevant tokens (comments, trailing whitespace, illegal tokens).
Assembly simply validates and puts together atoms and their kin.
Indentation detection manages only leading whitespace.
Sandhi is the most clearly context-sensitive pass, but it validates and adjusts tokens based on their immediate surroundings (one token left and/or right).
There _might_ be some room for re-ordering these passes, but this I think is the optimal ordering to reduce lookaround in each pass.

Future Work:
- [ ] validate that tabs do not exist oouotside of indentation
- [ ] enhance error monads to include more data about their cause and context
- [ ] write my own stream decoder, so I can track byte positions during lexing/parsing

### Standardization

I've decided to do the work of standardization in iterative passes.
While what I have so far isn't _neat_, I think it manages to at least describe what the lexer needs to do.

Future Work:
- [ ] consider the utility of dots/colons outside chaining/pairing
- [ ] consider backslash_symbol for a "named hole"
- [ ] formalize a generative grammar (ie maps valid CCS data types into character streams, but the inverse map states parsing _into_ useful data (asopposed to pass/fail))

## Source Code Library

This has, of course, data types for positions and spans inside source code.
It also has a small parser combinator library for extracting prefixes from existing source code, including position data.
It has helped me dramatically when creating several lexers: both the coverage lexer, and when analyzing leading whitespace.
I suspect it could be useful downstream of CCS, for example in languages where there are additional restrictions on the names of identifiers.

Future Work:
- [ ] positions should hold byte offsets
- [ ] positions need to be more unicode/rendering aware, in case of characters that aren't exactly one column wide

## Meta-work

I've given some thouoght to how I'm going to approach building this code up over time.
Previous codebases along these lines have gotten bogged down after initial excitement.
I hope by keeping a devlog, supported by a sketchier lab notebook would allow me to track (and appreciate!) my progress.
Additionally, I've started an audit script to keep an eye on some obvious issues without having to use a full (and non-portable) issue tracker.

Just today, I've been wondering about how to have a portable issue tracker.
That is, a tool that could analyze/manipulate a project folder/repo to manage issues within itself.
That would allow independence from github/gitlab/&c, and perhaps reduce context switching between the code base and a central issue repo.
Downside is that it would be less familiar, currently has no tooling, and it'd have to have a model for managing issues across branches.
This could be a good place to trial something, if I'm in the mood.
I did find some prior work: https://github.com/johann-petrak/simple-issues-tracker.

Future Work:
- [ ] write scripts to check if my labbook/blog are up to date
- [ ] integrate the audit scripts with githoks
- [ ] generalize audit scripts and package with nix
- [ ] explore in-repo issue tracking

## A New List-like Type

It's now deleted from the main branch, but consider `data Linear a r = Cons a (Linear a r) | Nil r`.

Remember how unary numbers are isomorphic to `List ()`?
Just generalize the `Succ Nat` constructor to `Succ a Nat(a)` (spelled `Cons a List(a)`).
Well, let's also generalize `Zero` to `Zero z`, and spell it `Done z`.
This gives `Linear a z = More a Linear(a, z) | Done z`.
We see that `Linear a ()` is isomorphic to lists, and `Linear () ()` is iso to `Nat`.
The basic idea is "let's iterate, and when we get to the end, also give back some extra data".
It neatly avoids a lot of the "tail-recur while accumulating to a buffer, then return a tuple of the fixed-up buffer and whatever other result I need from the iteration" junk I've done.
Note also that `Linear a r` is iso to `Streaming (Of a) Identity r`.

Future Work:
- [ ] implement the linear type in my extra lists package, as a low-power substitute for `Streaming`
    - alternately, submit it to the `streaming` package directly
